PLEASE CHANGE THE ARXIV, REPO, AND OTEHR URL in `agentflow/pyproject.toml`

# AgentFlow: In-The-Flow Agentic System Optimization for Effective Planning and Tool Use.

## Installation
please revise the project path in `setup.sh`, if you are not currently at the root of the project, and then run:

```bash
bash setup.sh
source .venv/bin/activate
# (Optional) Install `parallel` for running benchmark experiments in parallel:
sudo apt-get update
sudo apt-get install parallel
```

Make .env file, and set `OPENAI_API_KEY`, `GOOGLE_API_KEY`, `GOOGLE_CX`, etc. For example:
```text
# The content of the .env file

# Used for LLM-powered modules and tools
OPENAI_API_KEY=<your-api-key-here> # If you want to use OpenAI LLM
DASHSCOPE_API_KEY=<your-api-key-here> # If you want to use Gemini LLM

# Used for the Google Search tool
GOOGLE_API_KEY=<your-api-key-here>
GOOGLE_CX=<your-cx-here>

# Used for the more custom llm-engine (Optional)
TOGETHER_API_KEY=<your-api-key-here> # If you want to use TogetherAI LLM
DEEPSEEK_API_KEY=<your-api-key-here> # If you want to use DeepSeek LLM
XAI_API_KEY=<your-api-key-here> # If you want to use Grok LLM
ANTHROPIC_API_KEY=<your-api-key-here> # If you want to use Anthropic LLM
```

## Quick Start
### Dataset Preparation
Please run the following commands to fetch the data:
```bash
# train data
python data/get_train_data.py
# validation data
python data/aime24_data.py
```

The data dir should be:
```
data/
â”œâ”€â”€ train/
â”‚   â””â”€â”€ combined_train.parquet (182,190 samples)
â”œâ”€â”€ val/
â”‚   â””â”€â”€ aime24.parquet (30 samples)
â”œâ”€â”€ aime24_data.py
â””â”€â”€ get_train_data.py
```
### Train
**Start training with tmux:**
```bash
# Create tmux session and start agentflow service (Window 0)
tmux new-session -s agentflow
bash train/serve_with_logs.sh

# Create new window (Ctrl+B then C) and start training (Window 1)
bash train/train_with_logs.sh
```
**Configuration:**
All training hyperparameters are in `train/config.yaml` (model settings, tools, PPO parameters, resources, etc.)











## Test your env before going on

vplease run the following command to test all tools:

```bash
cd agentflow/agentflow
bash ./tools/test_all_tools.sh
```

A `test.log` will be saved in each tool's file. 

Success example: 
```text
Testing all tools
Tools:
  - base_generator
  - google_search
  - python_coder
  - web_search
  - wikipedia_search

Running tests in parallel...
Testing base_generator...
âœ… base_generator passed
Testing google_search...
âœ… google_search passed
Testing python_coder...
âœ… python_coder passed
Testing wikipedia_search...
âœ… wikipedia_search passed
Testing web_search...
âœ… web_search passed

âœ… All tests passed
```

### IP test
test your public IP(just for saving the logs files)
```bash
python util/get_pub_ip.py
```

### LLM engine test
Please run the following command to test all LLM engines:

```bash
python agentflow/scripts/test_llm_engine.py
```

Example output:
```text
ğŸš€ Starting fault-tolerant test for 11 engines...
ğŸ§ª Testing: 'gpt-4o' | kwargs={}
âœ… Success: Created ChatOpenAI
ğŸ§ª Testing: 'dashscope-qwen2.5-3b-instruct' | kwargs={}
âœ… Success: Created ChatDashScope
ğŸ§ª Testing: 'gemini-1.5-pro' | kwargs={}
âœ… Success: Created ChatGemini
============================================================
ğŸ“‹ TEST SUMMARY
============================================================
âœ… Passed: 3
   â€¢ gpt-4o â†’ ChatOpenAI
   â€¢ dashscope-qwen2.5-3b-instruct â†’ ChatDashScope
   â€¢ gemini-1.5-pro â†’ ChatGemini
âŒ Failed: 8
   â€¢ azure-gpt-4 â†’ ğŸš« API key not found in environment
   â€¢ claude-3-5-sonnet â†’ ğŸš« API key not found in environment
   â€¢ deepseek-chat â†’ ğŸš« API key not found in environment
   â€¢ grok â†’ ğŸš« API key not found in environment
   â€¢ vllm-meta-llama/Llama-3-8b-instruct â†’ ğŸš« Connection failed
   â€¢ together-meta-llama/Llama-3-70b-chat-hf â†’ ğŸš« API key not found
   â€¢ ollama-llama3 â†’ ğŸš« Connection failed
   â€¢ unknown-model-123 â†’ ğŸ’¥ Unexpected error
============================================================
ğŸ‰ Testing complete. Script did NOT crash despite errors.
```

## Quick Start
### Train

We recommend using `tmux` to manage the training process with two windows:

1. **Create a tmux session with two windows:**
```bash
tmux new-session -s agentflow
# This creates window 0 automatically
```

2. **In Window 0 - Start the VLLM serving:**
```bash
bash train/serve_with_logs.sh
```
This will start the VLLM server to serve your base model for rollout generation during training.

3. **Create and switch to Window 1 - Start the training:**
```bash
# Press Ctrl+B then C to create a new window
# Or manually: Ctrl+B then :new-window
bash train/train_with_logs.sh
```
This will start the RL training process.

**Switching between windows:**
- `Ctrl+B` then `0` - Switch to window 0 (serving)
- `Ctrl+B` then `1` - Switch to window 1 (training)
- `Ctrl+B` then `D` - Detach from session
- `tmux attach -t agentflow` - Reattach to session

**Training hyperparameters:**

You can customize training hyperparameters according to your needs. All configurations are defined in `train/config.yaml`, including:
- Model settings (`BASE_MODEL`, `ROLLOUT_TP_SIZE`)
- Tool configurations (`ENABLE_TOOLS`, `TOOL_ENGINE`)
- Training parameters (batch size, learning rate, PPO settings)
- Data paths and lengths
- Resource allocation (GPUs, workers)

For detailed parameter descriptions and tuning options, please refer to `train/config.yaml`.

### Infer

To run inference on benchmark tasks, first ensure your model is being served via VLLM, then execute:
```bash
cd test
bash exp/run_all_models_all_datasets.sh
```

**Serving models with VLLM:**

An easy VLLM serving script can be found in `scripts/serve_vllm.sh`. This script automatically launches multiple models in parallel using tmux:

```bash
bash scripts/serve_vllm.sh
```

Before running, configure the script:
- **models**: List of model paths to serve
- **gpu_groups**: GPU allocation for each model (e.g., `"0,1"` for 2 GPUs)
- **start_port**: Starting port number (default: 8000)

The script will create a tmux session and serve each model on consecutive ports (8000, 8001, etc.) with automatic tensor parallelism based on GPU count. 

**Configuration:**
Before running, configure the script in `test/exp/run_all_models_all_datasets.sh`:
- **TASKS**: Enable/disable tasks by commenting/uncommenting (e.g., `"aime24"`, `"gameof24"`, `"bamboogle"`)
- **MODELS**: Define models with their tool configurations:
  ```bash
  MODELS=(
      "8000:vllm-IPF/AgentFlow-3B,AgentFlow-3B,Base_Generator_Tool|Python_Coder_Tool,dashscope|dashscope"
      "8001:vllm-IPF/AgentFlow-7B,AgentFlow-7B,Base_Generator_Tool|Python_Coder_Tool,dashscope|dashscope"
  )
  ```
  Format: `"port:model_path,label,tools(|-separated),engines(|-separated)"`
- **THREADS**: Number of parallel workers (default: 20)

**Results location:**
After completion, results will be organized as follows:
```
test/
â””â”€â”€ {TASK_NAME}/           # e.g., aime24, gameof24, bamboogle
    â”œâ”€â”€ logs/
    â”‚   â””â”€â”€ {MODEL_LABEL}/  # e.g., AgentFlow-7B
    â”‚       â”œâ”€â”€ 0.log       # Individual problem logs
    â”‚       â”œâ”€â”€ 1.log
    â”‚       â””â”€â”€ ...
    â”œâ”€â”€ results/
    â”‚   â””â”€â”€ {MODEL_LABEL}/
    â”‚       â”œâ”€â”€ finalresults_direct_output.json   # Detailed results with analysis
    â”‚       â”œâ”€â”€ final_scores_direct_output.json   # Final scores and statistics
    â”‚       â”œâ”€â”€ finalscore_direct_output.log      # Scoring process log
    â”‚       â”œâ”€â”€ output_0.json              # Individual problem outputs
    â”‚       â”œâ”€â”€ output_1.json
    â”‚       â””â”€â”€ ...
    â””â”€â”€ cache/              # Cached intermediate results
```

**Key result files:**
- `final_scores_direct_output.json`: Contains accuracy, correct count, wrong PIDs, and tool usage statistics
- `finalresults_direct_output.json`: Detailed results with per-problem analysis and verification
- Individual `output_{i}.json`: Full output including query, response, memory, and execution traces

## Training Logs and Outputs

### Training Logs
During training, logs are automatically saved with IP-based organization:
```
task_logs/
â””â”€â”€ {PUBLIC_IP}/
    â””â”€â”€ train_log/
        â”œâ”€â”€ training_output_0000  # First 1MB of logs
        â”œâ”€â”€ training_output_0001  # Next 1MB
        â”œâ”€â”€ training_output_0002
        â””â”€â”€ ...
```
- Logs are split into 1MB files for easier management (configurable in `train/train_with_logs.sh`)
- Maximum 5000 log files retained
- Monitor latest logs: `tail -f task_logs/{YOUR_IP}/train_log/training_output_*`

### Model Checkpoints
Trained model checkpoints are saved periodically:
```
checkpoints/
â””â”€â”€ {PROJECT_NAME}/           # e.g., AgentFlow_general (from config.yaml)
    â””â”€â”€ {EXPERIMENT_NAME}/    # e.g., rollout_all_7B_useklloss (from config.yaml)
        â”œâ”€â”€ global_step_2/
        â”‚   â”œâ”€â”€ actor/
        â”‚   â”‚   â””â”€â”€ huggingface/  # HuggingFace format (ready for inference)
        â”‚   â””â”€â”€ data.pt           # Training state
        â”œâ”€â”€ global_step_4/
        â”œâ”€â”€ global_step_6/
        â””â”€â”€ latest_checkpointed_iteration.txt  # Points to latest checkpoint
```
**Checkpoint settings** (in `train/config.yaml`):
- `trainer.save_freq`: Checkpoint frequency (default: every 2 epochs)
- `trainer.test_freq`: Validation frequency (default: every 2 epochs)
- `trainer.total_epochs`: Total training epochs (default: 5)

### Rollout Data
During training, rollout trajectories are saved for analysis(start from 0 for each restart, the actual step may be different):
```
rollout_data/
â””â”€â”€ {PUBLIC_IP}/
    â””â”€â”€ {EXPERIMENT_NAME}_{TIMESTAMP}/     # e.g., rollout_all_7B_{time_stamp}
        â”œâ”€â”€ .init.lock
        â”œâ”€â”€ .run_info
        â””â”€â”€ {MODEL_NAME}_{TIMESTAMP}/      # e.g., Qwen2.5-7B-Instruct_{time_stamp}
            â”œâ”€â”€ train/                      # Training rollouts (usually empty to save space)
            â””â”€â”€ validation/
                â”œâ”€â”€ .val.lock
                â””â”€â”€ step_0/                 # Validation at global step 0
                    â”œâ”€â”€ idx_0/              # Individual validation samples
                    â”‚   â””â”€â”€ rollout_{uuid}.json
                    â”œâ”€â”€ idx_1/
                    â””â”€â”€ ...
```

**Rollout JSON structure** (each `rollout_{uuid}.json`):
- `prompt`: Original problem/query
- `groundtruth`: Expected answer
- `answer_extracted`: Model's extracted answer
- `reward`: Reward score (0.0 for incorrect, positive for correct)
- `total_result`: Complete execution trace including:
  - `query_analysis`: Problem analysis
  - `memory`: Step-by-step tool execution history
  - `direct_output`: Final model response
  - Tool prompts and responses for each step
- `timestamp`: Rollout generation time

**Using saved checkpoints:**
The models in `checkpoints/{PROJECT}/{EXPERIMENT}/global_step_X/actor/huggingface/` can be used for:
1. **Inference via VLLM**: Configure model paths in `scripts/serve_vllm.sh`
2. **Direct loading**: Standard HuggingFace Transformers `from_pretrained()`


